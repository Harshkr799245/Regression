{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Bac-pMmvItGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - A technique that models the relationship between a dependent variable (Y) and one independent variable (X) using a straight line:\n",
        "  - Y = mX + c\n",
        "\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  Key Assumptions of Simple Linear Regression:\n",
        "\n",
        "  - Linearity: The relationship between X and Y is linear\n",
        "  - Independence: Observations are independent of each other\n",
        "  - Homoscedasticity: Constant variance of residuals across all levels of X\n",
        "  - Normality: Residuals are normally distributed\n",
        "  - No outliers: Extreme values don't unduly influence the model\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "  -    The coefficient m is the slope of the regression line. It represents the average change in Y for each one-unit increase in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "  - The intercept c is the value of Y when X equals zero. It's where the regression line crosses the Y-axis.  \n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - The slope is calculated using: m = Σ[(Xi - X̄)(Yi - Ȳ)] / Σ[(Xi - X̄)²]\n",
        "Where X̄ and Ȳ are the means of X and Y respectively.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "  - The least squares method finds the best-fitting line by minimizing the sum of squared residuals (differences between actual and predicted values). This ensures the line is as close as possible to all data points collectively.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "  -\n",
        "R² represents the proportion of variance in the dependent variable explained by the independent variable.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "  - A regression model with two or more independent variables predicting the dependent variable.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  -   Simple regression uses one predictor variable, while multiple regression uses two or more predictor variables simultaneously. Multiple regression can capture more complex relationships and typically provides better predictions.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "  - Linearity\n",
        "\n",
        "  - Independence\n",
        "\n",
        "  - Homoscedasticity\n",
        "\n",
        "  - Normality of errors\n",
        "\n",
        "  - No multicollinearity\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results?\n",
        "  - Heteroscedasticity occurs when the variance of residuals changes across different levels of independent variables. Effects include:\n",
        "\n",
        "  - Standard errors become unreliable\n",
        "  - Confidence intervals and hypothesis tests become invalid\n",
        "  - Coefficients remain unbiased but inefficient   \n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "  - Improving models with high multicollinearity:\n",
        "\n",
        "    - Remove highly correlated variables\n",
        "    - Use Ridge or Lasso regression\n",
        "    - Apply Principal Component Analysis (PCA)\n",
        "    -Combine correlated variables into composite scores\n",
        "    - Collect more data to reduce correlation  \n",
        "\n",
        "13.  What are some common techniques for transforming categorical variables for use in regression model?\n",
        "  - One-hot encoding\n",
        "\n",
        "  - Label encoding\n",
        "\n",
        "  - Ordinal encoding (if categories have order)    \n",
        "\n",
        "14.  What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "  - Interaction terms capture how the effect of one variable depends on the level of another variable. For example, X₁ × X₂ shows whether the impact of X₁ on Y changes based on the value of X₂.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "  -   Intercept interpretation differences:\n",
        "\n",
        "  - Simple regression: Y-value when X = 0\n",
        "  - Multiple regression: Y-value when all independent variables equal zero (often not meaningful in practice)\n",
        "\n",
        "16.  What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "  - The slope quantifies the strength and direction of relationships between variables. It's crucial for:\n",
        "\n",
        "  - Understanding variable importance\n",
        "Making predictions\n",
        "  - Determining effect sizes\n",
        "  -Comparing impact across different variables\n",
        "\n",
        "  -In predictions, slopes determine how much the predicted value changes when input variables change, making them essential for scenario analysis and decision-making.\n",
        "\n",
        "17.  How does the intercept in a regression model provide context for the relationship between variables?\n",
        "  - In a regression model, the intercept provides context by representing the predicted value of the dependent variable when all independent variables are zero. It essentially tells you where the regression line crosses the y-axis.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "  - Doesn't penalize for overfitting\n",
        "\n",
        "  - Doesn't show if relationships are statistically significant\n",
        "\n",
        "  - Can be high even for a poor model\n",
        "\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "  - A large standard error for a regression coefficient indicates high uncertainty in the estimate. This suggests the coefficient is imprecisely estimated, possibly due to insufficient data, multicollinearity, or high variability in the data.\n",
        "\n",
        "\n",
        "20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "  - n residual plots, heteroscedasticity appears as a funnel or cone shape where residuals spread out (or contract) as fitted values increase. You might see patterns like increasing variance with larger predictions. It's crucial to address because it violates the constant variance assumption, leading to inefficient estimates and incorrect standard errors, which affects hypothesis testing and confidence intervals.\n",
        "\n",
        "21.  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - This indicates the model includes too many variables relative to the sample size, with many variables likely being irrelevant. The adjusted R² penalizes for the number of predictors, so this pattern suggests overfitting where the model fits the training data well but may not generalize effectively.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "  -caling ensures all variables contribute proportionally to the model, prevents variables with larger scales from dominating, stabilizes numerical computations, and makes coefficient interpretation more meaningful when variables have different units or ranges.\n",
        "\n",
        "23.  What is polynomial regression?\n",
        "  - Polynomial regression is a type of regression analysis where the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial. It extends linear regression to capture non-linear relationships by fitting a curve to the data instead of a straight line\n",
        "\n",
        "24.  How does polynomial regression differ from linear regression?\n",
        "  - hile linear regression assumes straight-line relationships, polynomial regression can model curved relationships. However, it's still \"linear\" in the coefficients, meaning the parameters appear linearly in the equation.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "  - Apply polynomial regression when scatter plots reveal curved patterns, residual analysis of linear models shows systematic patterns, or domain knowledge suggests non-linear relationships exist\n",
        "\n",
        "\n",
        "26.  What is the general equation for polynomial regression?\n",
        "  - General Equation: For a single variable: y = β₀ + β₁x + β₂x² + β₃x³ + ... + βₙxⁿ + ε\n",
        "\n",
        "27.  Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, polynomial regression can handle multiple variables by including polynomial terms for each variable and their interactions, though complexity increases rapidly with the number of variables and degree.\n",
        "\n",
        "\n",
        "28.  What are the limitations of polynomial regression?\n",
        "  - Polynomial regression is prone to overfitting with high degrees, can be unstable at boundaries, requires careful degree selection, may not extrapolate well beyond the training data range, and becomes computationally expensive with multiple variables.\n",
        "\n",
        "29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "  - R² / Adjusted R²\n",
        "\n",
        "  - Cross-validation\n",
        "\n",
        "  - AIC/BIC\n",
        "\n",
        "  - Residual analysis\n",
        "\n",
        "30.   Why is visualization important in polynomial regression?\n",
        "  - Plots help identify the appropriate degree, reveal overfitting through comparison of training vs. validation curves, show model behavior at boundaries, and communicate results effectively to stakeholders.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "  -\n",
        "\n",
        "```\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nqpa5J_2Iv9R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjGMp51FQ26D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}